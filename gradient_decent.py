# -*- coding: utf-8 -*-
"""Gradient_decent(Final).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bogdoY5Zid69TDMFcKUvb9YWSHFKJcn8

### **Question 6 Part 1**
"""

# Commented out IPython magic to ensure Python compatibility.
#importing important libraries
# %pylab inline

def func_β(β):
  return β ** 2

Start_point_range = -5
Stop_point_range = 5

Beta = np.linspace(Start_point_range, Stop_point_range)

plt.figure(figsize=(5,5))
plt.plot(Beta, func_β(Beta))
plt.xlabel('β'), plt.ylabel('func_β(β)')
plt.show()

# Define the beta der function and return the new beta value
def get_newβ(β, l_r):
  β_drev = - (2 * β)
  new_β = β + (l_r * β_drev)
  return new_β

def _gradient(β_new, β_old, stop_point, Learning_rate):
  β_l = []
  y_l = []
  while abs(β_new - β_old) > stop_point:
    # Update the β value
    β_old = β_new
    β_new = get_newβ(β_old, Learning_rate)
    β_l.append(β_new)
    y_l.append(func_β(β_new))
    plt.figure(figsize=(2,2))
    plt.scatter(β_l,y_l,c="orange")
    plt.plot(β_l,y_l,c="orange")
    plt.plot(Beta, func_β(Beta), c="grey")
    plt.title("Gradient descent")
    plt.show()
   

  print(f"Gradient found local minimum at: {β_new}")
  print(f"Gradient took {len(β_l)} steps")
  print(f"Learning Rate is : {Learning_rate}")

  plt.figure(figsize=(2,2))
  plt.scatter(β_l,y_l,c="orange")
  plt.plot(β_l,y_l,c="orange")
  plt.plot(Beta, func_β(Beta), c="grey")
  plt.title("Gradient descent")
  plt.show()

β_initial = 4.5
β_old = 0

_gradient(β_initial, β_old, 0.001, 0.05)

lr = [0.001, 0.003, 0.05, 0.06, 0.07 ]
for l in lr:
  _gradient(β_initial, β_old, 0.001, l)

"""### **Question 6 Part 2**"""

def S_func_β(β):
  β = (np.sin(10 * np.pi *β )/ (2 * β)) + ((β - 1) ** 4)
  return β

Start_point_range_s = 0.5
Stop_point_range_s = 2.5

S_beta = np.linspace(Start_point_range_s, Stop_point_range_s)

plt.figure(figsize=(4,4))
plt.plot(S_beta, S_func_β(S_beta))
plt.xlabel('S_β'), plt.ylabel('S_func_β(β)')
plt.show()

def get_S_newβ(β, l_r):
  β_drev = - (10 * np.pi * β *np.cos(10 * np.pi * β)-np.sin(10 * np.pi * β))/(2*(β ** 2)) +4 * ((β-1)**3)
  new_β = β + (l_r * β_drev)
  return new_β

def S_gradient(β_new, β_old, stop_point, Learning_rate):
  β_l = []
  y_l = []
  while abs(β_new - β_old) > stop_point:
    # Update the β value
    β_old = β_new
    β_new = get_newβ(β_old, Learning_rate)
    β_l.append(β_new)
    y_l.append(S_func_β(β_new))
    plt.figure(figsize=(2,2))
    plt.scatter(β_l,y_l,c="orange")
    plt.plot(β_l,y_l,c="orange")
    plt.plot(S_beta, S_func_β(S_beta), c="grey")
    plt.title("Gradient descent")
    plt.show()

  print(f"Gradient found local minimum at: {β_new}")
  print(f"Gradient took {len(β_l)} steps")
  print(f"Learning Rate is : {Learning_rate}")

  plt.figure(figsize=(7,7))
  plt.scatter(β_l,y_l,c="orange")
  plt.plot(β_l,y_l,c="orange")
  plt.plot(S_beta, S_func_β(S_beta), c="grey")
  plt.title("Gradient descent")
  plt.show()

S_β_initial = 2.4
S_β_old = 0

S_gradient(S_β_initial, S_β_old, 0.01, 0.05)

lr = [0.001, 0.003, 0.05, 0.06, 0.07 ]
for l in lr:
  S_gradient(S_β_initial, S_β_old, 0.01, l)

